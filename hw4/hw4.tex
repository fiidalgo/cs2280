\documentclass[11pt]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{thmtools}
\usepackage{bbm}

\declaretheorem{theorem}
\declaretheorem[style=definition]{problem}
\declaretheorem[style=remark, numbered=no]{hint}
\declaretheorem[style=remark, numbered=no]{note}

\newcommand*{\C}{{\mathcal C}}
\newcommand*{\Hp}{{\mathcal H}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\Z}{\mathbb{Z}}

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\1}{\mathbbm{1}}
\DeclareMathOperator{\Ber}{Ber}
\DeclareMathOperator{\size}{size}
\DeclareMathOperator{\VCD}{VCD}



\begin{document}

\begin{flushright}
			Leslie G. Valiant \\
			TFs: Aayush Karan \& Kevin Cong
\end{flushright}

\begin{center}
\textbf{CS 228: Computational Learning Theory} \\
Homework 4 (Take Home Exam) Problems\\ \textbf{Mar. 28th - Apr. 4th}
\end{center}

\textbf{Policy reminders:} You are strongly encouraged to type your solutions using LATEX. You may \textit{not} discuss this problem set with any one except the course staff. You are not allowed to use any materials except your own class notes, the course textbooks, and any material posted on the class website or handed out in class. Moreover, you are not allowed to query any LLM regarding any part of any problem. This problem set is due on \textbf{April 4}; \textit{no late days are allowed}. For any clarifications or questions, please send an email to the teaching staff. 


\rule{\linewidth}{0.4pt}


\begin{problem} [10 pts] \textbf{Few Relevant Variables Implies Efficient PAC Learnability.}
    
A variable $x_i$ is said to be \textit{relevant} to a boolean function $f$ if there are two truth assignments $A, B$ which differ only in their assignment to $x_i$ but which have $f(A) \neq f(B)$. Let $\mathcal{C}$ be the class of boolean functions over $\{0,1\}^n$ that have at most $\log n$ relevant variables. Give a direct proof (i.e., without reducing to other classes) that $\mathcal{C}$ is PAC-learnable in polynomial time by membership queries. 


\end{problem}

\paragraph{Problem Setup:} Let $f:{0,1}^n \to {0,1}$ be a Boolean function depending on at most $\log n$ variables. We give a direct polynomial-time learning algorithm using membership queries:

\paragraph{Identify relevant variables:} For each $i\in{1,\ldots,n}$, we test whether $x_i$ is relevant to $f$. Fix some reference assignment $a\in{0,1}^n$ (for example, $a=(0,0,\dots,0)$) and obtain $f(a)$ via a membership query. Then for each $i$, flip the $i$-th bit of $a$ to get $a^{(i)}$ (so $a^{(i)}_j = a_j$ for $j\neq i$ and $a^{(i)}_i = 1- a_i$) and query $f(a^{(i)})$. If $f(a^{(i)}) \neq f(a)$, then $x_i$ is relevant; if $f(a^{(i)}) = f(a)$, then $x_i$ is probably irrelevant. Since $f$ has at most $\log n$ relevant variables, most coordinates will appear irrelevant under any single assignment. If a truly relevant variable $x_i$ happens not to change $f$ when flipped (i.e. $f(a^{(i)}) = f(a)$ despite $x_i$ being relevant), it means that the particular assignment to the other relevant variables in $a$ caused $f$ to be insensitive to $x_i$. In that case, we can repeat the test under a different assignment. Because there are at most $\log n$ relevant variables, a random choice of $a$ will, with high probability, assign the other relevant bits in a way that reveals $x_i$â€™s influence. In fact, by trying at most $O(\log n)$ different random assignments as bases, we can ensure that every relevant coordinate flips the output in at least one trial. Thus, we can find the exact set $R \subseteq {1,\ldots,n}$ of relevant indices using at most $O(n \log n)$ queries.

\paragraph{Learn $f$ on relevant coordinates:} Let $r = |R| \le \log n$. Now $f$ is effectively a function of the $r$ bits in $R$. We can determine $f$ exactly by querying all $2^r$ possible assignments to those $r$ bits. For each $y \in {0,1}^R$ (an assignment to the bits in $R$), construct any $x \in {0,1}^n$ such that $x_R = y$ (bits in $R$ set according to $y$) and $x_j = 0$ for $j \notin R$ (arbitrary values work for irrelevant bits). Query $f(x)$ to obtain the value $f(y)$ (since bits outside $R$ do not matter). Collecting these results for all $2^r \le 2^{\log n} = n$ assignments, we now have the truth-table of $f$ as a function of its relevant inputs.

\paragraph{Conclusion:} Finally, output any hypothesis $h$ that depends only on $R$ and matches this truth-table (for example, a DNF or decision tree consistent with those $2^r$ values). By construction, $h(x) = f(x)$ for every $x \in {0,1}^n$. The algorithm uses at most $n\log n + n = O(n \log n)$ membership queries and runs in poly$(n)$ time. Therefore, $\mathcal{C}$ is PAC-learnable (in fact, exactly learnable) in polynomial time with membership queries.

\begin{problem} [15 pts] \textbf{Boolean Threshold Functions Are Not PAC-learnable.} Given any $y \in \{0,1\}^n$ and any integer $k \geq 0$, the boolean threshold function $TH_{k,y}$ is defined as follows: an input $x \in \{0,1\}^n$ is a positive example iff $x \cdot y \geq k$, where $x \cdot y$ denotes the standard real-valued dot product of two $n$-dimensional vectors. Prove that if $\mathsf{NP} \neq \mathsf{RP}$ then the representation class of boolean threshold functions is not PAC learnable by boolean threshold functions.

\textbf{Hint.} Reduce from the Zero-One Integer Programming problem (ZIP) which is known to be $\mathsf{NP}$-complete. An instance of ZIP is a set of $s$ pairs $\langle c_i, b_i \rangle$ and the pair $\langle \bar{a}, B \rangle$, where $c_i \in \{0,1\}^n$, $\bar{a} \in \{0,1\}^n$, $b_i \in \{0,1\}$, and $0 \leq B \leq n$. The problem is to determine whether there exists a vector $\bar{d} \in \{0,1\}^n$ such that $c_i \cdot \bar{d} \leq b_i$ for $1 \leq i \leq s$ and $\bar{a} \cdot \bar{d} \geq B$.

\end{problem}


\begin{problem}[15 pts] \textbf{Exact Learning DNFs via Membership Queries.} Prove that the class of $(\log n)$-term DNF is learnable in polynomial time in the exact learning model using membership and equivalence queries.

\textbf{Hint:} Reduce this problem to that of learning a DFA (deterministic finite automaton).
\end{problem}


\begin{problem} (16pt) \textbf{Teaching dimension.} 
Let $X$ be a finite instance space.  Given a concept class ${\cal C}$ and a
target concept $c \in {\cal C},$  we say that a sequence $T$ of labelled
examples is a {\em teaching sequence} for $c$ in ${\cal C}$ if $c$ is the only
concept in ${\cal C}$ that is consistent with $T$. Let $T(c)$ be the set of all
teaching sequences for $c$ in ${\cal C}.$  The {\em teaching dimension} of
concept class ${\cal C}$ is then defined to be
%
\[ \mbox{TD}({\cal C}) = \max_{c \in {\cal C}} \min_{T \in T(c)} |T| \] 
%
where $|T|$ denotes the number of examples in the sequence $T$.
\begin{enumerate}
\item (4pt) Give an example of a concept class ${\cal C}$ for which TD$({\cal C})
> $ VC-dim$({\cal C}).$
\item (4pt) Give an example of a concept class ${\cal C}$ for which TD$({\cal C})
< $ VC-dim$({\cal C}).$
\item (4pt) Show that for any concept class ${\cal C},$ TD$({\cal C}) \leq |{\cal
C}|-1.$
\item (4pt) Show that for any concept class ${\cal C},$ TD$({\cal C}) \leq $
VC-dim$({\cal C}) + |{\cal C}|- 2^{\mbox{\tiny{VC-dim}} ({\cal C})}$.
\end{enumerate}

\end{problem}


\begin{problem} [15 pts] \textbf{Persistent Classification Noise Model.}
For learning from noisy examples using membership queries, the random \textit{persistent} classification noise model is as follows: Given concept $c \in \mathcal{C}$ and noise rate $\eta$, a noisy concept $c'$ is produced by flipping the label of $c$ at each point with probability $\eta$. Then a learning algorithm gets all examples and membership queries labeled according to $c'$. This implies that in this model, with certain (albeit negligible) probability, $c'$ might be very far from $c$. For simplicity, we only require that a learning algorithm succeeds with probability at least $1/2$, where the probability of success also depends on the random choice of $c'$, and, as before, the running time can be polynomial in 
\[
\frac{1}{1-2\eta}.
\]
Let $\mathcal{P}$ be the class of parity functions over $\{0,1\}^n$ and $\mathcal{U}$ be the uniform distribution over $\{0,1\}^n$.

\begin{enumerate}
    \item[(a)] (5pts) Give an algorithm that learns $\mathcal{P}$ using membership queries in the presence of random persistent classification noise (that is, for any $\eta < 1/2$).
    \item[(b)] (5pts) Now assume that $c'$ is chosen by an adversary such that 
    \[
    \mathbf{Pr}_{\mathcal{U}}[c'(x) \neq c(x)] \leq \eta.
    \]
    Give an algorithm that learns $\mathcal{P}$ using membership queries in the presence of such malicious noise of rate $\eta = 1/5$.
    \item[(c)] (5pts) Prove that $\mathcal{P}$ is not learnable using membership queries with the malicious noise as above of rate $\geq 1/4$.
\end{enumerate}
\end{problem}
\end{document}